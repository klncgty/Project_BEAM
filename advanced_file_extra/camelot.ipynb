{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot.io as camelot\n",
    "from camelot import plotting\n",
    "import tkinter\n",
    "import os\n",
    "import json \n",
    "import warnings\n",
    "\n",
    "\n",
    "#ignoring warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"camelot.parsers.stream\")\n",
    "\n",
    "# loading our pdf via camelot\n",
    "tables = camelot.read_pdf('2024-TR_UNIT_PRICES_ENG.pdf',pages=\"2-end\", flavor='stream')\n",
    "\n",
    "print(f\"Data from the PDF has been converted into {tables.n} tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables.export('foo.csv', f='csv', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[10].df   ## data converted some tables. and we can check 10th table  for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[132].parsing_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at shapes of tables that we created\n",
    "\n",
    "for i in range(0,720):\n",
    "    print(f\"table[{i}] is  {tables[i].df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting json file from pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We turn our pdf to csv above codes. \n",
    "#    NOW IN THIS CELL,  \n",
    "#      we are getting a json source belongs to our pdf.\n",
    "\n",
    "all_tables = []\n",
    "\n",
    "for i in range(720):\n",
    "    temp_json_path = f\"jsons/temp_{i}.json\"\n",
    "    tables[i].to_json(temp_json_path)\n",
    "    \n",
    "    with open(temp_json_path, 'r') as file:\n",
    "        table_dict = json.load(file)\n",
    "        if table_dict: \n",
    "            all_tables.append(table_dict)\n",
    "    \n",
    "    os.remove(temp_json_path) \n",
    "\n",
    "with open(\"all_tables.json\", \"w\") as json_file:\n",
    "    json.dump(all_tables, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the codes below are about editing the generated CSV files. What we did in the cell above was to convert all the tables into a single JSON file that we got from the PDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the source data \n",
    "for i in range(3,582):\n",
    "    tables[i].df = tables[i].df.drop(index=0).reset_index(drop=True)\n",
    "    tables[i].df = tables[i].df.rename(columns={0: \"id\", 1: \"description\", 2: \"unit\", 3: \"price\"})\n",
    "for i in range(1,582):\n",
    "    if \"id\" in tables[i].df.columns:\n",
    "        tables[i].df = tables[i].df.drop(columns=['id'])\n",
    "for i in range(1, 582):\n",
    "    if len(tables[i].df.columns) == 2:\n",
    "        tables[i].df = tables[i].df.rename(columns={'unit': 'price'})\n",
    "        \n",
    "for i in range(1, 582):\n",
    "    if len(tables[i].df.columns) == 4:\n",
    "        tables[i].df = tables[i].df.rename(columns={'price': 'purchased at', 4: 'price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].df = tables[0].df.drop(index=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].df =  tables[0].df.rename(columns={0: \"id\", 1: \"description\", 2: \"unit\", 3: \"price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[300].df.to_csv(\"cleaned_data.txt\", index=False, sep='\\t')\n",
    "\n",
    "print(\"The DataFrame was successfully saved to the cleaned_data.txt file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_price_indices = []\n",
    "for i in range(1, 583):  # 1'den 582'ye kadar\n",
    "    if 'price' not in tables[i].df.columns:\n",
    "        missing_price_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for i in missing_price_indices:\n",
    "    tables[i].df = tables[i].df.rename(columns={0: \"id\", 1: \"description\", 2: \"unit\", 3: \"price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 583):\n",
    "    if i not in missing_price_indices:# 1'den 582'ye kadar\n",
    "        tables[i].df['price'] = tables[i].df['price'].astype(str) + \" TRY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 583):  # 1'den 582'ye kadar\n",
    "    df = tables[i].df\n",
    "    if 'price' in df.columns:\n",
    "        \n",
    "        df['price'] = df['price'].apply(lambda x: \"\" if x.strip() == \"TRY\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame başarılı bir şekilde cleaned_data.txt dosyasına kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "tables[300].df.to_csv(\"cleaned_data.txt\", index=False, sep='\\t')\n",
    "\n",
    "print(\"The DataFrame was successfully saved to the cleaned_data.txt file..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = \"cleaned_data.csv\"\n",
    "output_file = \"structured_output.txt\"\n",
    "with open(input_file, mode='r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    with open(output_file, mode='w', encoding='utf-8') as txt_file:\n",
    "        current_heading = None\n",
    "        for row in csv_reader:\n",
    "            if len(row) == 1 and row[0]:  # This row is a heading\n",
    "                current_heading = row[0]\n",
    "                txt_file.write(f\"{current_heading}\\n\")\n",
    "            elif len(row) > 1:  # This row is data under the current heading\n",
    "                if current_heading:\n",
    "                    txt_file.write(f\"    {row[0]} - {row[1]} - {row[2]}\\n\")\n",
    "\n",
    "                    \n",
    "print(\"Conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_folder = 'text_files'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for i in range(1, 583):\n",
    "    df = tables[i].df\n",
    "    \n",
    "    text = df.to_string()\n",
    "    \n",
    "    file_name = f'{output_folder}/dataframe_{i}.txt'\n",
    "    \n",
    "    # Metin dosyasına yazarken UTF-8 kodlaması kullanın\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "print(\"All DataFrames have been converted to text files and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "output_folder = 'text_files'\n",
    "collected_file_name = 'collected.txt'\n",
    "\n",
    "with open(collected_file_name, 'w', encoding='utf-8') as collected_file:\n",
    "    for i in range(1, 583):\n",
    "        file_name = f'{output_folder}/dataframe_{i}.txt'\n",
    "        \n",
    "        try:\n",
    "            with open(file_name, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                collected_file.write(f'--- Content of dataframe_{i}.txt ---\\n')\n",
    "                collected_file.write(content)\n",
    "                collected_file.write('\\n\\n') \n",
    "        except FileNotFoundError:\n",
    "            print(f'{file_name} not found, this file has been skipped.')\n",
    "\n",
    "print(f\"All text files are consolidated into a single file: {collected_file_name}\")\n",
    "\n",
    "for i in range(1, 583):\n",
    "    file_name = f'{output_folder}/dataframe_{i}.txt'\n",
    "    try:\n",
    "        os.remove(file_name)\n",
    "        print(f'{file_name} silindi.')\n",
    "    except FileNotFoundError:\n",
    "        print(f'{file_name} not found, this file may have already been deleted.')\n",
    "\n",
    "print(\"All text files have been deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
